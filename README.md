# Enhancing-FBSNN-Performance-for-HJB-Equations-Two-Methodologies-and-a-Failure-Analysis

This study addresses a key limitation of the Deep Forward-Backward Stochastic Neural Network (Deep FBSNN) method when solving high-dimensional Partial Differential Equations (PDEs), specifically the 100-dimensional Hamilton-Jacobi-Bellman (HJB) equation. While the FBSNN accurately predicts the solution at the boundary points (Y0, YT), a clear prediction discrepancy was observed within the internal time trajectory t → (0, T ). Two modification strategies—a structural enhancement of the neural network (e.g., FourierMultiDeepONet) and a dynamic loss function re-weighting technique—were attempted to improve trajectory accuracy but failed to yield significant improvements. This failure was primarily attributed to the inability of architectural enhancements to introduce the fundamental change in function representation necessary for handling the strong nonlinearity, and the instability caused by the dynamic shifting of the input data due to the simulation of Stochastic Differential Equation (SDE) paths. This failure underscores a structural limitation: the existing FBSNN framework has an inherent bias that causes a decrease in accuracy despite loss minimization. The analysis demonstrates that the FBSNN framework, due to its stochastic and path-dependent nature, requires a more fundamental, non-local approach to error distribution than effective static grid-based loss balancing. This finding highlights the strong need for subsequent research into core structural changes for robust FBSNN performance in high-dimensional problems.
